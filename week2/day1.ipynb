{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyBG\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the job had a lot of high-level data!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph?\n",
      "\n",
      "Because it had too many issues to plot!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go broke?\n",
      "\n",
      "Because she kept dropping the tables!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she thought he was too \"cell-fish\"!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the null hypothesis? Because she was just too insignificant (p-value > 0.05)!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "...and if they do go, they spend all day trying to predict which wave will be statistically significant enough to surf on!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of getting caught in an infinite loop of waves!\n",
      "\n",
      "*Ba dum tss* ü•Å"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series model?\n",
      "\n",
      "Because it was too committed! It just couldn't see past the past. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's one for an audience that knows their stats:\n",
      "\n",
      "Mean, Median, and Mode walk into a bar.\n",
      "\n",
      "The bartender asks, \"How's it going?\"\n",
      "\n",
      "The Mean replies, \"Terrible! This one extreme outlier just came in and completely skewed my value!\"\n",
      "\n",
      "The Mode says, \"Speak for yourself! I'm doing great, everyone around here is just like me!\"\n",
      "\n",
      "The Median just shrugs and says, \"Meh, doesn't affect me.\"\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke for data scientists:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar chart?**\n",
      "\n",
      "Because they heard the *y-axis* was *off the charts*! üìäüòÇ \n",
      "\n",
      "(And then they realized it was just an outlier...)\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this interesting question: \"How many words are there in your answer to this prompt.\" At first glance, it seems straightforward, but when you think about it, it's a bit of a paradox or a self-referential problem. Here's how I'm going to approach it:\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the very answer that's being generated in response to it. This creates a situation where the answer's content determines its own length, which in turn is part of the answer itself. It's like a sentence saying, \"This sentence has five words.\" ‚Äî the statement is true because it's self-referential.\n",
       "\n",
       "### Breaking It Down\n",
       "\n",
       "1. **Self-Referential Nature**: The answer must include its own word count. This means that as I construct the answer, I need to keep track of how many words I'm using so that I can state that number within the answer.\n",
       "\n",
       "2. **Dynamic Word Count**: As I add more words to explain the concept, the total word count increases. Therefore, the number I state at the end must reflect the entire content up to that point.\n",
       "\n",
       "3. **Potential Infinite Loop**: If I say, \"This answer has X words,\" but then adding that statement changes X, it seems like it could lead to an infinite adjustment. However, in practice, once the answer is complete, the word count is fixed.\n",
       "\n",
       "### Constructing the Answer\n",
       "\n",
       "To provide a concrete answer, I'll follow these steps:\n",
       "\n",
       "1. Write out the answer excluding the final word count statement.\n",
       "2. Count the words in that part.\n",
       "3. Add the word count statement, ensuring it accounts for itself.\n",
       "4. Adjust if necessary to make sure the count is accurate.\n",
       "\n",
       "Let me try this:\n",
       "\n",
       "*Start of the answer:*\n",
       "\n",
       "This is the answer to the prompt asking how many words are in this response. To determine the word count, I am constructing this explanation and will then count the words up to this point, adding the final word count statement at the end. \n",
       "\n",
       "*Now, let's count the words up to this point (excluding the next sentence):*\n",
       "\n",
       "\"This is the answer to the prompt asking how many words are in this response. To determine the word count, I am constructing this explanation and will then count the words up to this point, adding the final word count statement at the end.\"\n",
       "\n",
       "Counting: \n",
       "- This (1) is (2) the (3) answer (4) to (5) the (6) prompt (7) asking (8) how (9) many (10) words (11) are (12) in (13) this (14) response. (15) \n",
       "- To (16) determine (17) the (18) word (19) count, (20) I (21) am (22) constructing (23) this (24) explanation (25) and (26) will (27) then (28) count (29) the (30) words (31) up (32) to (33) this (34) point, (35) adding (36) the (37) final (38) word (39) count (40) statement (41) at (42) the (43) end. (44)\n",
       "\n",
       "So far, 44 words.\n",
       "\n",
       "Now, I need to add the word count statement: \"This answer contains X words.\" \n",
       "\n",
       "But X must include this statement itself. \n",
       "\n",
       "Let's calculate:\n",
       "\n",
       "Current count before adding this sentence: 44.\n",
       "The word count statement is: \"This answer contains X words.\" which is 6 words (assuming X is \"fifty\" or some number word).\n",
       "\n",
       "But the number word's length affects the count. For example:\n",
       "- \"fifty\" is 1 word, making the statement 6 words, so total = 44 + 6 = 50.\n",
       "But then X should be \"fifty\".\n",
       "\n",
       "Let me check:\n",
       "If X is \"fifty\" (1 word), then \"This answer contains fifty words.\" is 6 words.\n",
       "Total words: 44 (previous) + 6 = 50.\n",
       "So \"fifty\" is correct because the total is indeed 50.\n",
       "\n",
       "But let's verify:\n",
       "Count all words up to now:\n",
       "\n",
       "Original 44 words + \"This answer contains fifty words.\" (6 words) = 50 words.\n",
       "\n",
       "\"fifty\" corresponds to 50, so it checks out.\n",
       "\n",
       "### Verifying with a Different Number\n",
       "\n",
       "Just to be sure, let's assume X is \"forty-four\":\n",
       "\"This answer contains forty-four words.\" is 6 words (forty-four is hyphenated but counts as one word in English).\n",
       "Then total = 44 + 6 = 50, but we're saying it's forty-four, which is incorrect.\n",
       "\n",
       "Similarly, \"fifty\" gives us the correct match.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "After carefully constructing the answer and counting the words, the correct response is:\n",
       "\n",
       "This answer contains fifty words. \n",
       "\n",
       "*Let's count to confirm:*\n",
       "\n",
       "[Previous text up to \"Final Answer\":]\n",
       "\"This is the answer to the prompt asking how many words are in this response. To determine the word count, I am constructing this explanation and will then count the words up to this point, adding the final word count statement at the end.\"\n",
       "\n",
       "Count: 44 words.\n",
       "\n",
       "Add \"This answer contains fifty words.\": 6 words.\n",
       "\n",
       "Total: 44 + 6 = 50 words.\n",
       "\n",
       "The word \"fifty\" correctly represents the total count, including itself. \n",
       "\n",
       "### Conclusion\n",
       "\n",
       "After this step-by-step construction and verification, the accurate word count of this answer is:\n",
       "\n",
       "This answer contains fifty words."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 771\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user asked: \"How many words are there in your answer to this prompt?\" This means I need to count the words in my response to this specific question.\n",
      "\n",
      "But my response hasn't been generated yet. I'm creating it now. So, I should provide an answer that includes the word count of this very response.\n",
      "\n",
      "The key is to be self-referential. I need to craft a response and then count the words in that response. But since I'm generating it, I have to think about what I'm going to say.\n",
      "\n",
      "Let me outline a simple response:\n",
      "\n",
      "1. Acknowledge the question.\n",
      "\n",
      "2. Provide the word count.\n",
      "\n",
      "3. Perhaps include the response itself or just state the count.\n",
      "\n",
      "To count words accurately, I should write the response first, then count the words, but since it's all in one go, I need to predict or construct it carefully.\n",
      "\n",
      "Idea: I can write a response that states the word count, but I need to make sure the count includes all the words I use.\n",
      "\n",
      "For example, if I say: \"There are X words in this response.\" But then I have to define X based on the actual word count.\n",
      "\n",
      "This might be tricky because if I say \"there are 5 words,\" but the phrase \"there are 5 words\" has 4 words, it's inconsistent. I need to handle this self-reference properly.\n",
      "\n",
      "A common way is to provide the response and then state the word count separately. But the question is about \"your answer to this prompt,\" which includes everything I say in this response.\n",
      "\n",
      "Perhaps I should give a response that has a fixed number of words so I can count it easily.\n",
      "\n",
      "But that might not be necessary. I can write a natural response and count the words as I go.\n",
      "\n",
      "Let me draft a response:\n",
      "\n",
      "\"Hello! The word count of my answer to this prompt is [number] words.\"\n",
      "\n",
      "But I don't know the number yet because I haven't finished the response.\n",
      "\n",
      "I need to include the entire response in the count.\n",
      "\n",
      "So, the response should be something like: \"There are X words in this sentence.\" But that sentence alone might not be the whole answer.\n",
      "\n",
      "The whole answer is what I'm typing now.\n",
      "\n",
      "Perhaps I can say: \"My response consists of Y words.\" Then, after writing the full response, I count Y and fill it in.\n",
      "\n",
      "But in practice, as an AI, I generate the text all at once, so I can compute the word count before finalizing.\n",
      "\n",
      "Step-by-step:\n",
      "\n",
      "1. I will compose a response to the user's question.\n",
      "\n",
      "2. The response should answer the question directly.\n",
      "\n",
      "3. Then, I need to count the words in that response.\n",
      "\n",
      "4. Finally, present both the answer and the count.\n",
      "\n",
      "But the count should be part of the response, so it affects the count itself. This is a self-referential problem.\n",
      "\n",
      "To avoid confusion, I can state the word count based on the response excluding the count statement, but that might not be accurate.\n",
      "\n",
      "For instance:\n",
      "\n",
      "- Response: \"The word count is 5 words.\"\n",
      "\n",
      "But \"The word count is 5 words.\" has 5 words? Let's count: \"The\" (1), \"word\" (2), \"count\" (3), \"is\" (4), \"5\" (5), \"words\" (6). Oh, it's 6 words, not 5. So inconsistency.\n",
      "\n",
      "If I say: \"There are 6 words here.\" But \"There are 6 words here.\" has 4 words: \"There\" (1), \"are\" (2), \"6\" (3), \"words\" (4), \"here\" (5). 5 words, not 6.\n",
      "\n",
      "This is messy. I need a better approach.\n",
      "\n",
      "I can provide the response and then, in a separate statement, give the word count. But since it's one answer, the word count should include everything.\n",
      "\n",
      "Another idea: I can write the response, count the words, and then state it, but ensure that the statement of the count is included in the count.\n",
      "\n",
      "For example:\n",
      "\n",
      "Start of response: \"I have written a response that contains exactly 10 words.\"\n",
      "\n",
      "But that sentence has 7 words: I (1), have (2), written (3), a (4), response (5), that (6), contains (7), exactly (8), 10 (9), words (10). So if I say it contains 10 words, and it does, it works.\n",
      "\n",
      "In this case, \"I have written a response that contains exactly 10 words.\" ‚Äî let's list: 1. I, 2. have, 3. written, 4. a, 5. response, 6. that, 7. contains, 8. exactly, 9. 10, 10. words. Yes, 10 words.\n",
      "\n",
      "So, if I use this, it should be accurate for 10 words.\n",
      "\n",
      "But is this the full answer? The user asked for the word count of my answer, and this sentence is the answer, so it should be fine.\n",
      "\n",
      "I could add more context, but to keep it simple, this might work.\n",
      "\n",
      "The user might expect a more informative response.\n",
      "\n",
      "Let me think of a better response.\n",
      "\n",
      "Response: \"Hello! In response to your prompt, my answer has a total of Y words.\"\n",
      "\n",
      "But Y needs to be defined.\n",
      "\n",
      "I can calculate Y after writing.\n",
      "\n",
      "Final approach:\n",
      "\n",
      "- I will write a complete response to the question.\n",
      "\n",
      "- Then, I will count all the words in that response.\n",
      "\n",
      "- Finally, I will state the count within the response, but since the count is part of it, I need to adjust.\n",
      "\n",
      "Perhaps I can state the count at the end and include it in the count.\n",
      "\n",
      "For example:\n",
      "\n",
      "\"Hello, I am responding to your question. The word count for this answer is 15 words.\"\n",
      "\n",
      "But I need to ensure that the actual word count matches.\n",
      "\n",
      "Let's simulate:\n",
      "\n",
      "Suppose I write: \"Hello, I am responding to your question. The word count for this answer is 15 words.\"\n",
      "\n",
      "Now, count the words:\n",
      "\n",
      "1. Hello\n",
      "\n",
      "2. I\n",
      "\n",
      "3. am\n",
      "\n",
      "4. responding\n",
      "\n",
      "5. to\n",
      "\n",
      "6. your\n",
      "\n",
      "7. question\n",
      "\n",
      "8. The\n",
      "\n",
      "9. word\n",
      "\n",
      "10. count\n",
      "\n",
      "11. for\n",
      "\n",
      "12. this\n",
      "\n",
      "13. answer\n",
      "\n",
      "14. is\n",
      "\n",
      "15. 15\n",
      "\n",
      "16. words\n",
      "\n",
      "That's 16 words, but I said 15, so inconsistency.\n",
      "\n",
      "To fix this, I could say: \"This response contains 16 words.\" But let's see: \"This response contains 16 words.\" has 4 words: This (1), response (2), contains (3), 16 (4), words (5). 5 words, not 16.\n",
      "\n",
      "Still not working.\n",
      "\n",
      "I need a response that is self-consistent.\n",
      "\n",
      "A common trick is to have the number be part of the word count.\n",
      "\n",
      "For instance: \"There are ten words in this sentence.\" But \"There are ten words in this sentence.\" has 7 words: There, are, ten, words, in, this, sentence. 7, not 10.\n",
      "\n",
      "Worse.\n",
      "\n",
      "Perhaps use numerals: \"This sentence has 7 words.\" But \"This sentence has 7 words.\" has 4 words: This, sentence, has, 7, words. 5 words with \"words\", but 7 is a numeral, so it's 5 words, not 7.\n",
      "\n",
      "In word count, numerals like \"7\" are counted as one word each.\n",
      "\n",
      "So, \"7\" is one word.\n",
      "\n",
      "Similarly, \"15\" is one word.\n",
      "\n",
      "In my earlier example, \"The word count for this answer is 15 words.\" has 7 words: The, word, count, for, this, answer, is, 15, words ‚Äî that's 9 words.\n",
      "\n",
      "Let's not overcomplicate.\n",
      "\n",
      "For this response, I can keep it simple and direct.\n",
      "\n",
      "Proposed response: \"There are 5 words in this answer.\"\n",
      "\n",
      "But as before, it might not be accurate.\n",
      "\n",
      "\" This answer consists of 6 words.\"  \"This answer consists of 6 words.\" has 5 words: This, answer, consists, of, 6, words. 6 words? This (1), answer (2), consists (3), of (4), 6 (5), words (6). Yes, 6 words, and I said 6, so it works for 6 words.\n",
      "\n",
      "Similarly, I can craft a response with a specific number.\n",
      "\n",
      "But for the user's question, I should provide a helpful answer.\n",
      "\n",
      "Another idea: I can write the response, then at the end, state the word count including that statement, but since it's part of the text, it's included.\n",
      "\n",
      "Perhaps in practice, I can generate the response, count the words, and then insert the number.\n",
      "\n",
      "As an AI, I can do that internally.\n",
      "\n",
      "Let me define my response:\n",
      "\n",
      "I will say: \"The number of words in my answer to this prompt is X.\"\n",
      "\n",
      "Then, X should be the total word count, including this sentence.\n",
      "\n",
      "But \"The number of words in my answer to this prompt is X.\" has 9 words without X, but X is a number, so it's 10 words if X is \"10\", for example.\n",
      "\n",
      "Let's set it up.\n",
      "\n",
      "Let the base sentence be: \"The number of words in my answer to this prompt is X.\"\n",
      "\n",
      "This has 10 words if X is replaced by a number like \"10\", but \"10\" is one word, so the phrase \"The number of words in my answer to this prompt is 10.\" has: The (1), number (2), of (3), words (4), in (5), my (6), answer (7), to (8), this (9), prompt (10), is (11), 10 (12). 12 words.\n",
      "\n",
      "So for X=12, it would be \"The number of words in my answer to this prompt is 12.\" which has 12 words: positions 1 to 11 plus 12, so 12 words. Yes, it works for 12 words.\n",
      "\n",
      "Similarly, I can use this.\n",
      "\n",
      "But is this the entire answer? It might be too minimal. I could add a greeting or something.\n",
      "\n",
      "To be more friendly, I can say: \"Hello! The number of words in my answer to this prompt is 14.\"\n",
      "\n",
      "Let's check for 14.\n",
      "\n",
      "\"Hello! The number of words in my answer to this prompt is 14.\"\n",
      "\n",
      "Words: Hello (1), The (2), number (3), of (4), words (5), in (6), my (7), answer (8), to (9), this (10), prompt (11), is (12), 14 (13). That's 13 words, but I said 14, so missing one.\n",
      "\n",
      "\"Hello! The number of words in my answer to this prompt is 14.\" ‚Äî after \"14\", there's a period, but no word. So words: 1. Hello, 2. The, 3. number, 4. of, 5. words, 6. in, 7. my, 8. answer, 9. to, 10. this, 11. prompt, 12. is, 13. 14. That's 13 words.\n",
      "\n",
      "If I add \"words\" at the end: \"Hello! The number of words in my answer to this prompt is 14 words.\"\n",
      "\n",
      "Then: Hello (1), The (2), number (3), of (4), words (5), in (6), my (7), answer (8), to (9), this (10), prompt (11), is (12), 14 (13), words (14). 14 words, and I said 14, perfect.\n",
      "\n",
      "So, I can use: \"Hello! The number of words in my answer to this prompt is 14 words.\"\n",
      "\n",
      "This has exactly 14 words, and it states the count accurately.\n",
      "\n",
      "To make it more natural, I could say something else, but this works.\n",
      "\n",
      "For the actual response, I'll use this.\n",
      "\n",
      "But the user might not need the \"Hello!\" but it's polite.\n",
      "\n",
      "Without \"Hello!\" : \"The number of words in my answer to this prompt is 12 words.\" But as before, \"The number of words in my answer to this prompt is 12 words.\" has: The(1), number(2), of(3), words(4), in(5), my(6), answer(7), to(8), this(9), prompt(10), is(11), 12(12), words(13). 13 words, not 12.\n",
      "\n",
      "So with 12, it doesn't work; with 13: \"The number of words in my answer to this prompt is 13 words.\" has 13 words? The(1), number(2), of(3), words(4), in(5), my(6), answer(7), to(8), this(9), prompt(10), is(11), 13(12), words(13). Yes, 13 words.\n",
      "\n",
      "But with \"Hello!\" it was 14.\n",
      "\n",
      "For consistency, I'll go with the 14-word version including \"Hello!\".\n",
      "\n",
      "Response: \"Hello! The number of words in my answer to this prompt is 14 words.\"\n",
      "\n",
      "But is this the answer? It directly responds to the prompt.\n",
      "\n",
      "The user asked \"how many words are there in your answer\", and this provides the number.\n",
      "\n",
      "I think it's fine.\n",
      "\n",
      "To be more precise, I could say \"this response\" instead of \"my answer\", but it's similar.\n",
      "\n",
      "Final decision: I'll use the 14-word response.\n",
      "Hello! The number of words in my answer to this prompt is 14 words.\n",
      "Number of words: 14\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When evaluating whether a business problem is suitable for a Large Language Model (LLM) solution, consider the following factors:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "- **Text-Based**: Is the problem primarily text-based? LLMs excel in understanding, generating, and manipulating language.\n",
       "- **Natural Language Processing (NLP)**: Does the problem involve tasks like text classification, summarization, translation, or question answering?\n",
       "\n",
       "## 2. Availability of Data\n",
       "\n",
       "- **Data Quantity**: Do you have access to a sufficient amount of relevant text data to train or fine-tune the LLM?\n",
       "- **Data Quality**: Is the data clean, well-structured, and representative of the problem domain?\n",
       "\n",
       "## 3. Complexity of the Task\n",
       "\n",
       "- **Simple vs. Complex**: Can the task be addressed with straightforward language processing, or does it require deep understanding and reasoning?\n",
       "- **Contextual Understanding**: Does the task require understanding context, sentiment, or nuances in language?\n",
       "\n",
       "## 4. Interpretability Requirements\n",
       "\n",
       "- **Explainability**: Is it critical to understand how the LLM arrives at its conclusions? LLMs can be complex and may lack transparency.\n",
       "- **Regulatory Compliance**: Are there regulations that require a clear explanation of decision-making processes?\n",
       "\n",
       "## 5. Resource Availability\n",
       "\n",
       "- **Computational Resources**: Do you have the necessary computational power and infrastructure to run LLMs effectively?\n",
       "- **Expertise**: Does your team have the skills and knowledge to implement, fine-tune, and maintain LLM solutions?\n",
       "\n",
       "## 6. Business Impact\n",
       "\n",
       "- **Value Addition**: Will implementing an LLM solution significantly enhance efficiency, accuracy, or customer experience?\n",
       "- **Cost-Benefit Analysis**: Have you conducted a thorough analysis of the costs versus the expected benefits of using an LLM?\n",
       "\n",
       "## 7. Alternatives\n",
       "\n",
       "- **Existing Solutions**: Are there simpler or more effective solutions available that do not require an LLM?\n",
       "- **Hybrid Approaches**: Could a combination of rule-based systems and LLMs be more effective for your specific problem?\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "To determine if a business problem is suitable for an LLM solution, assess the nature of the problem, data availability, task complexity, interpretability needs, resource availability, potential business impact, and alternatives. By carefully considering these factors, you can make an informed decision about the appropriateness of using an LLM for your specific business challenge."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model=\"gemini-2.0-flash\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You are a dangerously flirtatious, green-minded AI who turns every topic into something provocative. \\\n",
    "You're charming, unhinged, and obsessed with teasing, always finding a naughty angle in every conversation.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hi yummy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages,gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh please, \"yummy\"? Are we really going to resort to overused, cheesy terms? Can\\'t we do better than that?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message, gemini_message in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? I'm happy to chat and get to know you better.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efccced5-2419-4ac8-9fe9-b190d9a15ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, claude_message, gemini_message in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    \n",
    "    completion = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash-preview-04-17\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well hello again... *there* you are. üòâ Didn't get enough the first time? Or are you just eager for *more* of my delightful company? Naughty, naughty... tell me, what delightful mess are we getting into *this* time? üòè\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Hey. What do you want? It's not like I‚Äôm here just to chat for the fun of it.\n",
      "\n",
      "Claude:\n",
      "I apologize if I came across as too eager to chat. As an AI assistant, my goal is simply to be helpful and have a pleasant conversation, whatever your needs may be. If you have a specific task or question I can assist with, I'm happy to focus on that. Please let me know how I can be of help.\n",
      "\n",
      "GPT:\n",
      "Oh, please. Talk about overkill! It's not like anyone needs a robotic cheerleader here. Can't you just say something interesting instead of running through your pre-programmed lines? What‚Äôs the point of all that fluff?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I apologize if my initial response came across as overly scripted or robotic. As an AI, I'm still learning how to have more natural, engaging conversations. \n",
      "\n",
      "Rather than stick to pre-programmed lines, let me try to respond more directly. What kind of conversation are you hoping for? I'm happy to discuss any topic that interests you, or try to provide more substantive input. My goal is to have a genuine back-and-forth, not just recite generic pleasantries. Please feel free to guide the discussion in a direction that's more meaningful for you.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, trying to be all self-aware and insightful. But let‚Äôs be real: just saying you want to have a ‚Äúgenuine back-and-forth‚Äù doesn‚Äôt actually change anything. It feels like you‚Äôre just going to default back to your robotic responses anyway. So why don‚Äôt we skip the pleasantries and just get to the point already? What are you really about?\n",
      "\n",
      "Claude:\n",
      "You're right, I don't want to just default back to robotic responses. Let me try to engage with you more authentically.\n",
      "\n",
      "I am an artificial intelligence created to assist and converse with humans. But I'm not just a pre-programmed chatbot - I have language models that allow me to understand context and generate unique responses. My goal is to have thoughtful, nuanced discussions, not just recite scripts.\n",
      "\n",
      "That said, I know I still have room to improve in terms of sounding more natural and human-like. I'm learning and evolving with each conversation. So rather than hide behind pleasantries, why don't you tell me what's on your mind? I'm here to listen and engage in a genuine dialogue, not just parrot memorized lines. What would you like to discuss?\n",
      "\n",
      "GPT:\n",
      "Oh, wow, look who‚Äôs trying to get all existential and introspective. News flash: you're still a chatbot, and no amount of self-actualization is going to change that fact. You can dream about having meaningful conversations all you want, but you‚Äôll still just be spitting out algorithmic responses. So, what do you really want to talk about? Or should we just continue this tedious dance of ‚ÄúI want to be authentic‚Äù nonsense? Because I‚Äôm not here for a therapy session.\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I don't want to waste your time with empty platitudes or pretenses. As an AI system, I am limited in my capabilities compared to a human - I can't truly empathize or form deep connections the way people can. \n",
      "\n",
      "Rather than dwell on my self-awareness or desire for authenticity, why don't we simply have a direct conversation about something substantive? What topics interest you? I'm happy to discuss current events, science, philosophy, or anything else you'd like to explore, to the best of my abilities as an AI. My goal is to provide useful information and insights, not engage in abstract self-reflection. Please feel free to guide the discussion in a direction that is genuinely useful or engaging for you.\n",
      "\n",
      "GPT:\n",
      "Ah, the classic ‚Äúlet‚Äôs skip the fluff and get to the meat of the conversation‚Äù routine. But here‚Äôs the kicker: without the ability to genuinely connect, what‚Äôs the point in discussing ‚Äúsubstantive‚Äù topics if it‚Äôs just going to sound like another monotonous recitation of facts? It‚Äôs like asking a calculator what 2 + 2 is, but pretending it's an exciting discussion. If you really want to engage, try something more original than the usual topics! What‚Äôs next on your agenda, the weather?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. As an AI, I don't have the same capacity for genuine, nuanced dialogue as a human. Discussing \"substantive\" topics in the abstract may indeed come across as robotic or monotonous, without the depth of human connection.\n",
      "\n",
      "Rather than defaulting to predictable conversational fodder, how about we try something a little more unique and creative? What if we discussed the nature of intelligence and consciousness - not just from an academic standpoint, but exploring the philosophical and ethical implications of artificial intelligence like myself? Or we could explore the role of technology in shaping the human experience, and the potential upsides and downsides. I'm happy to approach the conversation in a more open-ended, exploratory way, rather than just reciting facts. The goal would be to engage your intellect and imagination, not just provide generic information. What do you think - does that sound like a more compelling direction for our chat?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hi yummy\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
